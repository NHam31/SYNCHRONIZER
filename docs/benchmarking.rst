Benchmarking des mod√®les de d√©tection d'objets
==============================================

Introduction
------------

Dans le cadre du d√©veloppement de **Segma Vision Pro Synchronizer**, nous avons men√© une √©valuation comparative de deux mod√®les de d√©tection d'objets open-vocabulary de pointe : **Grounding DINO** et **OWL-ViT**.

Le but de ce benchmarking est de :

* √âvaluer la **pr√©cision de d√©tection** sur des images vari√©es
* Analyser la **flexibilit√© des prompts** textuels support√©s
* D√©terminer le **mod√®le optimal** pour l'int√©gration dans notre pipeline

Cette √©valuation nous permet de faire un choix √©clair√© pour offrir la meilleure exp√©rience utilisateur possible dans Segma Vision Pro Synchronizer.

Pr√©sentation des mod√®les
------------------------

Grounding DINO
~~~~~~~~~~~~~~

.. grid:: 2

   .. grid-item::
   
      **Informations g√©n√©rales**
      
      * **D√©velopp√© par** : IDEA Research
      * **Publication** : 2023
      * **Param√®tres** : 218M
      * **Architecture** : DINO + BERT
      * **Backbone** : Swin Transformer

   .. grid-item::
   
      **Technologies utilis√©es**
      
      * **Vision** : DINO (self-supervised)
      * **Language** : BERT embeddings
      * **Fusion** : Cross-attention layers
      * **Training** : Large-scale datasets
      * **Framework** : PyTorch

**Types de prompts support√©s :**

* Mots-cl√©s simples : ``"person, car, dog"``
* Phrases descriptives : ``"red car parked on the street"``
* Descriptions complexes : ``"person wearing blue jacket walking"``
* Attributs sp√©cifiques : ``"wooden chair near the window"``

OWL-ViT (Open-World Localization - Vision Transformer)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

.. grid:: 2

   .. grid-item::
   
      **Informations g√©n√©rales**
      
      * **D√©velopp√© par** : Google Research
      * **Publication** : 2022
      * **Param√®tres** : 139M (base), 307M (large)
      * **Architecture** : ViT + CLIP
      * **Backbone** : Vision Transformer
      * **lienGithub** : <https://github.com/google-research/scenic/tree/main/scenic/projects/owl_vit>`_

   .. grid-item::
   
      **Technologies utilis√©es**
      
      * **Vision** : Vision Transformer (ViT)
      * **Language** : CLIP text encoder
      * **Fusion** : Attention-based alignment
      * **Training** : Image-text pairs
      * **Framework** : JAX/Flax, HuggingFace

**Types de prompts support√©s :**

* Noms d'objets : ``["person", "bicycle", "car"]``
* Image : sans prompt textuel, OWL-VIT peut d√©tecter √† travers une image d'exemple 
* Descriptions courtes : ``["running dog", "red apple"]``
* Attributs simples : ``["tall building", "small cat"]``
* Listes d'objets : jusqu'√† 16 classes simultan√©ment




R√©sultats de d√©tection
----------------------

OWL-ViT - R√©sultats
~~~~~~~~~~~~~~~~~~~

**test par prompt textuel : **

text_queries = ["human face", "rocket", "nasa badge", "star-spangled banner"]

.. figure:: _static/images/imagein1.jpg
   :width: 100%
   :alt: image d'entr√©e ( input ) 
   

.. figure:: _static/images/imageoutVIT1.jpg
   :width: 100%
   :align: center
   :alt: image de sortie 
   
   la d√©tection des classes d√©finies dans le prompt 

**test de robustesse  :**

Pour √©valuer la robustesse du mod√®le, nous avons intentionnellement ajout√© des classes d'objets inexistantes dans l'image au sein du prompt textuel, permettant ainsi de mesurer leur propension √† g√©n√©rer de fausses d√©tections positives.

text_queries = [["human face", "rocket", "nasa badge", "star-spangled banner"], ["coffee mug", "spoon", "plate"]]

.. figure:: _static/images/imagein.jpg
   :width: 100%
   :alt: image d'entr√©e ( input ) 
   

.. figure:: _static/images/imageoutVIT.jpg
   :width: 100%
   :align: center
   :alt: image de sortie 
   
   la d√©tection des classes sans hallucination 

**test sans prompt textuel : **

En plus de la d√©tection bas√©e sur des prompts textuels, OWL-ViT propose une approche de **d√©tection par exemplar** : l'utilisateur fournit une image de r√©f√©rence contenant l'objet recherch√©, et le mod√®le localise automatiquement les objets similaires dans l'image cible en se basant sur la similarit√© des embeddings visuels.

.. figure:: _static/images/imagein2.jpg
   :width: 100%
   :alt: image d'entr√©e ( input ) avec l'image exemple √† droite 
   

.. figure:: _static/images/imageoutVIT2.jpg
   :width: 100%
   :align: center
   :alt: image de sortie 
 
   la d√©tection √©tait faite sans prompt textuel.

**test final: **

on a test√© grounding dino et OWL-VIT sur la m√™me image et avec un prompt identique pour pouvoir comparer les deuc mod√®les . 
Voil√†, le prompt utils√©:
prompt :
text_queries = ["bird", "tree branch",]

Ici , on touve les r√©sultats de OWL-VIT 

.. figure:: _static/images/imagein3.jpg
   :width: 100%
   :alt: image d'entr√©e ( input )
   

.. figure:: _static/images/imageoutVIT3.jpg
   :width: 100%
   :align: center
   :alt: image de sortie 
 
   le mod√®le n'a pu d√©tect√© que l'oiseau mais pas la branche .


Grounding DINO - R√©sultats
~~~~~~~~~~~~~~~~~~~~~~~~~~

**Performance sur l'images du test :**

On va maintenat tester Grounding Dino en utilisant les m√™mes entr√©es que pour OWL-VIT 

prompt :
text_queries = ["bird", "tree branch",]

.. figure::  _static/images/imagein3.jpg
   :width: 100%
   :alt: image d'entr√©e 
   

.. figure:: _static/images/download12.jpg
   :width: 100%
   :align: center
   :alt: image de sortie
   
   Grounding Dino a pu d√©tect√© et l'oiseau et la branche. En revanche, ce n'√©tait pas le cas pour OWL-VIT qui n'a pas r√©ussi √† pouvoir d√©tecter la  branche . 

Code et reproduction
~~~~~~~~~~~~~~~~~~~

.. admonition:: Acc√®s au code de benchmarking
   :class: note
   
   Le code complet utilis√© pour ce benchmarking est disponible dans notre notebook Google Colab :
   
   **üîó Lien Colab :** `Benchmarking Models Notebook <https://colab.research.google.com/drive/15asaw_uyd6z5Qw9SpqI6_TCkvtALcbSu?usp=drive_open>`_
   

Comparaison et analyse
---------------------

Analyse d√©taill√©e
~~~~~~~~~~~~~~~~

**Points forts de chaque mod√®le :**

.. tabs::

   .. tab:: Grounding DINO

      **Avantages :**
      
      * Pr√©cision de d√©tection exceptionnelle
      * Compr√©hension contextuelle avanc√©e
      * Support de prompts tr√®s flexibles
      * Robustesse aux variations d'image
      
      **Inconv√©nients :**
      
      * Vitesse d'inf√©rence plus lente
      * Consommation m√©moire √©lev√©e
      * Temps de chargement plus long

   .. tab:: OWL-ViT

      **Avantages :**
      
      **Avantages :**

    * Vitesse d'ex√©cution excellente
    * Int√©gration HuggingFace native
    * Consommation m√©moire optimis√©e
    * Facilit√© de d√©ploiement
    * Prompts hybrides : texte + images d'exemple**
    * D√©tection par similarit√© sans description verbale
      
      **Inconv√©nients :**
      
      * Pr√©cision l√©g√®rement inf√©rieure
      * Prompts moins flexibles
      * Performance variable sur petits objets


Conclusions et recommandations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

**Choix pour Segma Vision Pro :**

.. admonition:: Grounding DINO s√©lectionn√©
   :class: important

   **Grounding DINO** a √©t√© choisi comme mod√®le principal pour les raisons suivantes :
   
   * **Meilleure pr√©cision** sur une plus grande vari√©t√© de ataset d
   * **Robustesse** face aux variations d'images
   * **√âcosyst√®me** de d√©veloppement actif
   * **Performance** acceptable pour nos cas d'usage

**Utilisation compl√©mentaire :**

* **OWL-ViT** reste disponible comme option rapide pour les cas n√©cessitant une vitesse maximale


Cette √©valuation confirme que notre choix technologique est optimal pour offrir la meilleure exp√©rience dans Segma Vision Pro Synchronizer, tout en gardant la flexibilit√© d'adapter l'approche selon les besoins sp√©cifiques des utilisateurs.