{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYuf9dFF5CGz",
    "outputId": "52037544-c4d8-468a-a9e8-94b34a491393"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3gKc3GCz-Jc"
   },
   "source": [
    "### **INSTALLATION & SETUP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "k_XSfwtQBACY",
    "outputId": "37b6b240-4bbc-4059-f449-ffa785c6a14f"
   },
   "outputs": [],
   "source": [
    "# Step 1: Uninstall both numpy and opencv\n",
    "!pip uninstall -y numpy opencv-python opencv-python-headless\n",
    "\n",
    "# Step 2: Install compatible versions (remove redundant numpy installation)\n",
    "!pip install numpy==1.23.5 opencv-python==4.8.1.78 --quiet\n",
    "!pip install --no-cache-dir numpy==1.24.4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BItpzblIhZN3",
    "outputId": "f9661b7e-5a50-420b-9642-31314eac5ba0"
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y transformers huggingface_hub numpy\n",
    "!pip install transformers==4.39.3 numpy --upgrade --no-cache-dir\n",
    "!pip install accelerate torchvision --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfJToJB8hgYm",
    "outputId": "3c41b230-9a92-460f-fe7b-e64589d48c47"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4 --no-cache-dir --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPr8mE8WfkUY",
    "outputId": "3bcc93db-3250-4064-a0ce-f4f5ad587747"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVwBzfe4f1Ao",
    "outputId": "22162db3-08d0-4391-a76b-5a702d3c3ba4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-9VMchMLf4Rf",
    "outputId": "0930a5e3-94fc-40b3-b0a4-81a0e5787c1e"
   },
   "outputs": [],
   "source": [
    "!pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5vAnwa0QTQl"
   },
   "source": [
    "rerun this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTPuLmSCf9Zb"
   },
   "outputs": [],
   "source": [
    "\n",
    "!mkdir -p {HOME}/weights\n",
    "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aB15nd-8f_jx",
    "outputId": "a6d94e4d-b182-419e-84c5-6883e8e3fe55"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tu8prFIgCPW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CQn1x0njkII_",
    "outputId": "13827c86-0359-4438-b2f0-6b97246fd935"
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "\n",
    "# Initialize the SAM model\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgafv8rZgGS1"
   },
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WojcLW8niUux",
    "outputId": "7c0b0e52-e8c1-467d-ba74-6bb7e5249ab5"
   },
   "outputs": [],
   "source": [
    "!pip install supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ud8HD9CL4X_X",
    "outputId": "dfcaa8e4-51d9-43dd-93b1-6607a584481a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "# üîÅ 1. D√©finir le chemin\n",
    "IMAGE_PATH = \"/content/drive/MyDrive/Segme_Vision_Pro_Synchronizer/pic/berlin_322463_1.jpg\"\n",
    "\n",
    "# ‚úÖ 2. V√©rifier que le fichier existe\n",
    "assert os.path.exists(IMAGE_PATH), f\"‚ùå Fichier introuvable : {IMAGE_PATH}\"\n",
    "\n",
    "# üßæ 3. Lire l'image\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "assert image_bgr is not None, f\"‚ùå √âchec de chargement : {IMAGE_PATH}\"\n",
    "\n",
    "# üé® 4. Convertir en RGB (requis pour SAM)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# üß† 5. G√©n√©rer les masques avec SAM (supposant mask_generator d√©j√† d√©fini)\n",
    "sam_result = mask_generator.generate(image_rgb)\n",
    "\n",
    "print(f\"‚úÖ {len(sam_result)} masques g√©n√©r√©s avec succ√®s.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IAx-nq2Ogpcw",
    "outputId": "d242f7b3-aaac-4bbc-d868-5a3764585cde"
   },
   "outputs": [],
   "source": [
    "print(sam_result[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "R5IKCIqYgrkA",
    "outputId": "b942be67-be25-404b-a171-83a8d6fe5edd"
   },
   "outputs": [],
   "source": [
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[image_bgr, annotated_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1O1LOMEdLNB6",
    "outputId": "689d3518-244d-4f44-a352-370eeed75d27"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Boucle sur chaque masque\n",
    "for i, result in enumerate(sam_result):\n",
    "    mask = result['segmentation']  # bool√©en (H, W)\n",
    "\n",
    "    # Appliquer le masque √† l‚Äôimage\n",
    "    masked = image_rgb.copy()\n",
    "    masked[~mask] = 255  # Mettre le fond en blanc\n",
    "\n",
    "    # Afficher l‚Äôobjet extrait\n",
    "    plt.figure(figsize=(3,3))\n",
    "    plt.imshow(masked)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Objet {i}\")\n",
    "    plt.show()\n",
    "\n",
    "    # Optionnel : Sauvegarde de chaque objet\n",
    "    img = Image.fromarray(masked)\n",
    "    img.save(f\"/content/mask_object_{i}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsgJmbd95yql"
   },
   "source": [
    "\n",
    "**BLIP1**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "7466d1465c5f4bcb951df0cb70722438",
      "53ef76926d994ced979ef7e3fb0945ef",
      "f7bb4b1ba4e148e5a263117206f463ee",
      "244f0e0940ab40f3a6768598379137c8",
      "867f5d7283924493ab569b2fc592bced",
      "245ec8604bf7423287467083771c4271",
      "5b638ded9d454946b6028424e37cfdae",
      "a9f956dc9efa4938b67c827c559aa5d9",
      "ca9b7557447646e38f046d5dd758294b",
      "55998c0b90644ccc870093baad6a2494",
      "741cccc7447b4312920f66e31fd651d2",
      "0dfd42d7daa44e139bd2dcda8854a6f8",
      "daad45dc091641c9a99d9042b86bdb53",
      "13776b6d941643dda7576cbda9ad58d0",
      "3ff68fd5c08c4db9ae15133ae7deb8da",
      "85362a0320034ece9bc636c05416dda8",
      "94a04b85c78a4a5a8d55623a1c9e2147",
      "e5d28d42cbb948b7b09b241e046fabff",
      "31388b5e1f1e4bf29eff7d9ff93a672b",
      "232ca523a32a436286af86d435028f90",
      "1da22af951a94bca982f2b638c3fcc6c",
      "15a8bcabe65f4255adaceb91338c14bb",
      "e10204f0814d4e32b6176ac347618823",
      "77c2c617616e464a941e03c1154189c6",
      "ffaa3b32dc3e487795ffc7006399ccc5",
      "3c0a7a2ac493418987803e8a41e22d90",
      "894178fd8ab34b068d4bf11249629e68",
      "46c6343ef5bc466a9045ef01c05f2737",
      "16f5d217846047e5af21e7acd364b881",
      "550c4e401b7940c8a4b6ea52f26870a7",
      "3edfc2c66c20447586c013a43be5c4d0",
      "c7c27113b0254d6f9a0f439b59989b51",
      "a2d6014b36924fd6b4296faadad62e11",
      "e7bd1832380340bc99ce6f3b29444873",
      "b69aba1c217e4e2d81b87f0ddaf4cccf",
      "aec665649e1c446b84eaf7b396a5a464",
      "6e0b041c752b49d7a6a0911d8c0122f7",
      "92536ed9a0394ff689e59cd6215ef4b4",
      "0e2e94fd6a9c4c70a35919342e0c93f9",
      "46bd6b60a6c24c67935c5c81af44920c",
      "83f1e9dad8e0443786375e29bb6440c3",
      "01fddcb14270435ca225c3a558c6a095",
      "ba9b16bd6c674fa7a8adea2e74f44d4e",
      "5f876881966e433fb395f12dc958e808",
      "f326e72431bc443dafddb732cb013a6e",
      "9b87709c18834f6f81cc4a68b4857707",
      "d33f0ac162a54c8ead7982b91d10c1cd",
      "8db945a03f1641ac94083da7c32568b2",
      "97ef00b04eb645b19d85e683c9b3cdc5",
      "72cf9a4f37674642a1566c4befd9092c",
      "0807f1f7009b44b0a7173bf59c592184",
      "6edbe6687a8b457d8e9879e66d15a2d4",
      "5781635661d1470ba5db1416f24a1156",
      "98c2eec9f55e4b719f09fbafb1fd50df",
      "efc90b0948f64570af624aadfe07e402",
      "f29c4d6a31744cb9ad8e949fe14c5296",
      "239a3fbef9c84e1786cd651ca6a2b47d",
      "7a3d7c4d549c4e30b894f02165c2c110",
      "a7eab74d15364000b15ef4ecc554fd7c",
      "a39f7b6ff17f4e738f61b214ef519b45",
      "3d88ef5f65184758a1f07f39a788b5d7",
      "2a2f7f82100140b8bf336f5736b0d9d3",
      "a9324aacfe814641995f6eb22a7ca9d2",
      "15114c8c94ff4a64b3887cab1195a85e",
      "0987b6501a104045ab3ceff56015e494",
      "8a8e6503c6084fc58828c5e54c07e731",
      "84331689f7d04b89b4d4493bd2bd1bc1",
      "93559bdd5ba64a60aa5d6ce2c15b5791",
      "7946d70533944e30aa72fe5e60222d7d",
      "96b430b75a8b4c12b6ca124c0e23e06a",
      "75b9e051565342e389760165d0ac9275",
      "e3e86ddc9f2743c58bb8fe3c2b4270a1",
      "3446cab94081492bac1a3863609b0b42",
      "92996d9030724ac285f286cff2f26bb5",
      "fa7b6d3e6f17447ea613a62d06dc8978",
      "da8263effb73477db5cbc6b681a7f503",
      "131413befc55489388eaa8209b13d0d9"
     ]
    },
    "id": "GwkPsJyayCi8",
    "outputId": "3d308c0a-bebf-4b50-abec-15d68789d711"
   },
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Charger mod√®le BLIP (captioning)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "007_jE5rzeaq",
    "outputId": "c14c0e2a-0cd5-4073-afe6-beb5d2dfe081"
   },
   "outputs": [],
   "source": [
    "def describe_object(image_rgb, mask):\n",
    "    # Extraire l'objet masqu√© (fond blanc)\n",
    "    masked = image_rgb.copy()\n",
    "    masked[~mask] = 255\n",
    "\n",
    "    # Convertir en PIL Image\n",
    "    pil_img = Image.fromarray(masked)\n",
    "\n",
    "    # Pr√©parer l'entr√©e pour BLIP\n",
    "    inputs = processor(pil_img, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # G√©n√©rer la l√©gende\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# Exemple d'utilisation sur un masque\n",
    "for i, result in enumerate(sam_result):\n",
    "    mask = result[\"segmentation\"]\n",
    "\n",
    "    caption = describe_object(image_rgb, mask)\n",
    "    print(f\"Objet {i} : {caption}\")\n",
    "\n",
    "from supervision import Detections, MaskAnnotator, ColorLookup\n",
    "\n",
    "# G√©n√©rer les captions pour chaque masque\n",
    "labels = [describe_object(image_rgb, result[\"segmentation\"]) for result in sam_result]\n",
    "\n",
    "# Cr√©er les d√©tections avec les captions comme labels\n",
    "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "detections.labels = labels  # Assigne directement les labels\n",
    "\n",
    "\n",
    "# Annoter l'image avec les noms d'objets\n",
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i5qfD22Z0N8E",
    "outputId": "00b3fe49-6a55-4474-e2e4-23eee4138065"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import supervision as sv\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "# --- 1. Charger et pr√©parer l'image ---\n",
    "IMAGE_PATH = \"/content/drive/MyDrive/Segme_Vision_Pro_Synchronizer/pic/berlin_322463_1.jpg\"\n",
    "assert os.path.exists(IMAGE_PATH), f\"‚ùå Fichier introuvable : {IMAGE_PATH}\"\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "assert image_bgr is not None, f\"‚ùå √âchec de chargement : {IMAGE_PATH}\"\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# --- 5. Fonction pour g√©n√©rer la description d'un objet masqu√© ---\n",
    "def describe_object(image_rgb, mask):\n",
    "    masked = image_rgb.copy()\n",
    "    masked[~mask] = 255  # Fond blanc\n",
    "    pil_img = Image.fromarray(masked)\n",
    "    inputs = processor(pil_img, return_tensors=\"pt\").to(device)\n",
    "    out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n",
    "\n",
    "# --- 6. G√©n√©rer toutes les annotations (captions) ---\n",
    "labels = []\n",
    "for i, result in enumerate(sam_result):\n",
    "    mask = result[\"segmentation\"]  # masque bool√©en\n",
    "\n",
    "    # Image objet avec fond blanc\n",
    "    masked = image_rgb.copy()\n",
    "    masked[~mask] = 255  # fond blanc\n",
    "    mask = result[\"segmentation\"]\n",
    "    caption = describe_object(image_rgb, mask)\n",
    "    labels.append(caption)\n",
    "    # Affichage\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(masked)\n",
    "    plt.title(f\"Objet {i} : {caption}\", fontsize=10)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Cr√©er les d√©tections avec les captions comme labels\n",
    "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "detections.labels = labels  # Assigne directement les labels\n",
    "\n",
    "\n",
    "# Annoter l'image avec les noms d'objets\n",
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtCBHv7o29h8",
    "outputId": "57494125-5e34-4b1f-9e5b-aabdfe77ad52"
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- 7. Regrouper toutes les annotations en une seule phrase ---\n",
    "if len(labels) > 1:\n",
    "    phrase_complete = \"; \".join(labels[:-1]) + \" et \" + labels[-1]\n",
    "elif labels:\n",
    "    phrase_complete = labels[0]\n",
    "else:\n",
    "    phrase_complete = \"\"\n",
    "\n",
    "print(\"\\n--- Phrase compl√®te regroupant toutes les annotations ---\\n\")\n",
    "print(phrase_complete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "y0MsLguU8QDa"
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "LhaMPPOx8R-N"
   },
   "outputs": [],
   "source": [
    "!pip install mistral_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VWVaEDFV8Uaa"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"YOUR_TOKEN_HERE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164,
     "referenced_widgets": [
      "4e6ed740e11c4a97bccfe8a8a3f508e3",
      "e3a2d2b1f10b4a2eb9767d154e983b1d",
      "e89e5f9d1ecf4d9e929fbe2dac24f37b",
      "9c5a237931a644ddaa5710073dd01e51",
      "519da91c1b864618985125f7a64ae887",
      "8aabd9a3ec164a4eaae079c7941fd3cc",
      "31909e9bec0348e29d49665258f4a1ad",
      "4213e2c0324c437f8b563023957f79cb",
      "bea9b49e29414427894ad0a7dc1a7dae",
      "68a1fb95fe054336bccb6285077a53d3",
      "77e7e6825ef940d5810ba73daa6c77f9",
      "6869d571d05848f095251df54d2f87e6",
      "d75e5cffce834d53a57075c4737dcd52",
      "cc3fd7064e2d445cb51fef4aadbe4779",
      "0a5c5c8f9a03495c9406ffefdeba3e2a",
      "884b09ea717545dbaa2ab3f63892c2ff",
      "435c5c2985f84836857639694968ed71",
      "0bf598f1e3ea4f959fd940d3bb392671",
      "7b9e97744cea45a88ff288499b40ba53",
      "cf0dae79d30248a1b78f0bc7dfd71c59",
      "3df35564800548b5bd52d2b538b98fd1",
      "e0afd4d8c845493480dcea2fa395c41e",
      "26672f188d624237a793b97766ae9236",
      "a50e835429ab45ccbc4843e755a451ab",
      "d76446531d644ed4840f038dd2087888",
      "4d79383f679d47ea8d6218f37979fb4f",
      "0b291fc2a5594807abd6be56540c6c12",
      "e128bce760854fa4babf9e23a25a5487",
      "a7224889f54c4d15875114bffea4bcce",
      "f5c9b909e20b40c7802ea76b99299775",
      "d3bf00eefcde4fde829bbc82bc0c6c18",
      "e403b4d06c3748e9bb0ab326b2f5c542",
      "996743dcfe2f4aa998648cc7607821ad",
      "3e7cf8dad8de4b70abafeedc06b48c37",
      "4bbc9868013b40c092de0f479ad5c349",
      "8fde1e9982354ade9f86fa2a981382e3",
      "9c9516b4eed147bcb349bff1ca63bc03",
      "9fc7bfd2a3d04af8813ac42e2e2f25f3",
      "d522e1fb0f48490ebd2344cfb0c7c5b2",
      "c8c0b71b83384a389d57dbe2e9790b46",
      "cc631e7e45d845b4a63c9ac127f7ff42",
      "4631106818124be5b5aaf65c0d75d004",
      "fe7f7f18a3724ad988d4d31df2961820",
      "d0297d7796a3495b9e3c9f0212dec8ba"
     ]
    },
    "id": "euPOAxTk8XBl",
    "outputId": "17672401-a30a-4409-97d2-1ad82a2f4a30"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "mistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\n",
    "mistral_models_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "snapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "vcpqMtPS8Y3M"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "EGdsvj4z8gIp"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51D6aFfY-52x"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121,
     "referenced_widgets": [
      "6a8ea655cb7640d9a3ae5a3c9707e9f1",
      "433c95298e7a408c8fa816a4cf12c897",
      "6935bcb59615435ab9a521745e97bc30",
      "1ae8c63b4b984f8793c5e2f791f9c178",
      "0be2db4e1cf441918ca6c98fd5194c0a",
      "d7bed93ee32c403f89e9ad3bde3a7c5c",
      "3bc69432561b462f83b0a6af8986eb22",
      "f8a4513ade1d4f40a6705d1211352f3c",
      "ee7b9baa32a840c7a24e5be00f453838",
      "63fc28a177094c77a53d57d89013b48f",
      "daac13e947504df0a30063cf0d603cbd"
     ]
    },
    "id": "o9WS7Qrajau6",
    "outputId": "eac4653e-a76c-4090-f07c-c43187b5cb9a"
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use. Infer this from the user's location.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\", \"format\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "conversation = [{\"role\": \"user\", \"content\": \"What's the weather like in Paris?\"}]\n",
    "\n",
    "\n",
    "\n",
    "# format and tokenize the tool use prompt\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tools=tools,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "inputs.to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=1000)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ht8DlaBA8dP0"
   },
   "outputs": [],
   "source": [
    "def extract_objects_with_mistral(description: str) -> list[str]:\n",
    "    prompt = (\n",
    "        \"You are an AI assistant that extracts visual objects from image descriptions.\\n\"\n",
    "        \"From the following text, list *only the main visible objects* (no colors, no adjectives, no duplicates).\\n\"\n",
    "        \"Output a comma-separated list in lowercase. End the list with a dot.\\n\\n\"  # ‚ú® Added instruction to end with a dot\n",
    "        f\"Description: {description}\\n\\n\"\n",
    "        \"Objects:\"\n",
    "    )\n",
    "\n",
    "    # Tokenize and send to model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "\n",
    "    # Decode output\n",
    "    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # üßº Improved extraction logic: stop at the first period\n",
    "    object_line = decoded.split(\"Objects:\")[-1].split(\".\")[0].strip()\n",
    "\n",
    "    # Clean and split the object list\n",
    "    object_list = [obj.strip().lower() for obj in object_line.split(\",\") if obj.strip()]\n",
    "\n",
    "    return list(set(object_list))  # Remove duplicates just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElmYu2La8sd8",
    "outputId": "bf33ff16-2e68-4bfb-f270-d67c13c4c24d"
   },
   "outputs": [],
   "source": [
    "description = \"a white light switch plate with a white background; thermorer‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢‚Ñ¢; a white plastic plate with a hole for the bottom; a white background with a white circle; a white and brown flower on a white background; a white wall light with a white shade; a white and brown table with a wooden top; a bed with a brown cover and a white pillow; a wooden door with a handle on it; a wooden shelf with a white background; a brown wooden table with a white background; a tall wooden cabinet with a door; a wooden table with a white background; a wooden shelf with a white background; a pair of white and black metal lamps; a wooden table with a white background; a small wooden object with a white background; a wooden door with a white background; a white background with a black and white border; a black and white floor lamp with a white shade; a white and brown flower on a white background; a white background with a brown and black pattern; a small wooden box with a handle; a white wall mounted light with a white background; a vase with branches on it; a white background with a wooden shelf; a black and white lamp with a white shade; a gold plate with a small square shaped object; a black and white photo of a man in a suit; a white background with a black and white image of a man in a suit; a pair of black and white chairs; the row of leather mules in brown; a small wooden block with a white background; a white and black floor lamp with a white shade; a small bird flying in the sky; a white background with a black and white image; a white background with a black and white image; a black and white photo of a wall light; a black and white photo of a wall light; a white background with a black and white image of a white background with a black and white image of et a white background with a black and white image of a black and white image of a black and white\"\n",
    "#print(description)\n",
    "objects = extract_objects_with_mistral(description)\n",
    "print(objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JRb9G7Zr5sqi"
   },
   "source": [
    "**GROUNDINGDINO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W3iCG8xekvKE",
    "outputId": "0ea33355-a098-4fdf-ad85-ab110ad6fc96"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8q13mQT9kw1W",
    "outputId": "b1d032f9-c5cb-4560-b0a8-fbb1bca0e56f"
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)  # Doit √™tre >= 4.41.0.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rppZ_C2A6eUt"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-tiny\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "from transformers import GroundingDinoForObjectDetection\n",
    "model = GroundingDinoForObjectDetection.from_pretrained(model_id).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kQbcIxGP7dn5",
    "outputId": "88d798fc-c937-4a8b-9aa4-71ffd09a9bfe"
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "image = Image.open(\"/content/drive/MyDrive/Segme_Vision_Pro_Synchronizer/pic/berlin_322463_1.jpg\").convert(\"RGB\")\n",
    "\n",
    "# Ton texte (phrase compl√®te) √† utiliser comme prompt\n",
    "text = objects  # Exemple, adapte selon ta phrase\n",
    "\n",
    "# Important: ajout de truncation et max_length\n",
    "inputs = processor(\n",
    "    images=image,\n",
    "    text=text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    max_length=256\n",
    ").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    box_threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "0Ab1RKUM_84y",
    "outputId": "14d7c700-b01c-4da7-a779-b1f9bec7f1d9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Afficher l'image\n",
    "fig, ax = plt.subplots(1, figsize=(12, 9))\n",
    "ax.imshow(image)\n",
    "\n",
    "# Dessiner les bo√Ætes autour des objets d√©tect√©s\n",
    "for result in results[0][\"boxes\"]:\n",
    "    box = result.tolist()  # Convertir la bo√Æte en une liste pour un format plus facile √† utiliser\n",
    "    x, y, w, h = box\n",
    "    rect = patches.Rectangle((x, y), w - x, h - y, linewidth=2, edgecolor=\"r\", facecolor=\"none\")\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UV74l-bJTgHK",
    "outputId": "d33db4d5-86b3-4b47-ee89-bad4a19c5cea"
   },
   "outputs": [],
   "source": [
    "'''import nbformat\n",
    "\n",
    "file = \"/content/synchronizer.ipynb\"\n",
    "\n",
    "# Open the notebook\n",
    "with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Check and delete 'widgets' from metadata\n",
    "if \"widgets\" in nb.metadata:\n",
    "    print(\"üîß Found 'metadata.widgets' ‚Äî removing it...\")\n",
    "    del nb.metadata[\"widgets\"]\n",
    "else:\n",
    "    print(\"‚úÖ No widgets found.\")\n",
    "\n",
    "# Save cleaned notebook\n",
    "with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"‚úÖ Notebook cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrtkHOOgY0yZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "khB28ft1Y1LF",
    "outputId": "0a7719fc-5655-423a-9539-13f8204cbe44"
   },
   "outputs": [],
   "source": [
    "'''import nbformat\n",
    "\n",
    "file = \"/content/synchronizer.ipynb\"  # name of your notebook\n",
    "\n",
    "# Open the notebook\n",
    "with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Check and delete 'widgets' from metadata\n",
    "if \"widgets\" in nb.metadata:\n",
    "    print(\"üîß Found 'metadata.widgets' ‚Äî removing it...\")\n",
    "    del nb.metadata[\"widgets\"]\n",
    "else:\n",
    "    print(\"‚úÖ No widgets found.\")\n",
    "\n",
    "# Save cleaned notebook\n",
    "with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"‚úÖ Notebook cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hR_thjQNblyo",
    "outputId": "588640fd-c3af-442b-e353-282c203fc008"
   },
   "outputs": [],
   "source": [
    "'''import json\n",
    "import sys\n",
    "\n",
    "def clean_notebook(notebook_path):\n",
    "    with open('/content/synchronizer.ipynb', 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    # Supprimer les m√©tadonn√©es de widgets au niveau du notebook\n",
    "    if 'widgets' in notebook.get('metadata', {}):\n",
    "        del notebook['metadata']['widgets']\n",
    "\n",
    "    # Nettoyer chaque cellule\n",
    "    for cell in notebook.get('cells', []):\n",
    "        # Supprimer les outputs des cellules\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "\n",
    "        # Supprimer les m√©tadonn√©es de widgets des cellules\n",
    "        if 'widgets' in cell.get('metadata', {}):\n",
    "            del cell['metadata']['widgets']\n",
    "\n",
    "        # R√©initialiser execution_count\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "    # Sauvegarder le notebook nettoy√©\n",
    "    with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(notebook, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Notebook {notebook_path} nettoy√© avec succ√®s!\")\n",
    "\n",
    "# Utilisation\n",
    "clean_notebook('votre_notebook.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mQSu_rs_dr_k",
    "outputId": "48d907c1-0fb8-4811-ba0f-b6a131584350"
   },
   "outputs": [],
   "source": [
    "'''import json\n",
    "import os\n",
    "\n",
    "def deep_clean_notebook(notebook_path):\n",
    "    print(f\"üîç Analyse de {notebook_path}...\")\n",
    "\n",
    "    with open('/content/synchronizer.ipynb', 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    changes_made = False\n",
    "\n",
    "    # 1. Nettoyer les m√©tadonn√©es au niveau du notebook\n",
    "    if 'metadata' in notebook:\n",
    "        if 'widgets' in notebook['metadata']:\n",
    "            print(\"‚ùå Suppression des m√©tadonn√©es widgets du notebook\")\n",
    "            del notebook['metadata']['widgets']\n",
    "            changes_made = True\n",
    "\n",
    "        # Supprimer aussi d'autres m√©tadonn√©es probl√©matiques\n",
    "        problematic_keys = ['widget_state', 'application/vnd.jupyter.widget-state+json']\n",
    "        for key in problematic_keys:\n",
    "            if key in notebook['metadata']:\n",
    "                del notebook['metadata'][key]\n",
    "                changes_made = True\n",
    "\n",
    "    # 2. Nettoyer chaque cellule\n",
    "    for i, cell in enumerate(notebook.get('cells', [])):\n",
    "        # Supprimer les outputs\n",
    "        if 'outputs' in cell and cell['outputs']:\n",
    "            print(f\"üßπ Nettoyage des outputs de la cellule {i}\")\n",
    "            cell['outputs'] = []\n",
    "            changes_made = True\n",
    "\n",
    "        # Supprimer execution_count\n",
    "        if 'execution_count' in cell and cell['execution_count'] is not None:\n",
    "            cell['execution_count'] = None\n",
    "            changes_made = True\n",
    "\n",
    "        # Nettoyer les m√©tadonn√©es des cellules\n",
    "        if 'metadata' in cell:\n",
    "            cell_metadata_keys_to_remove = ['widgets', 'widget_state', 'application/vnd.jupyter.widget-state+json']\n",
    "            for key in cell_metadata_keys_to_remove:\n",
    "                if key in cell['metadata']:\n",
    "                    print(f\"‚ùå Suppression des m√©tadonn√©es {key} de la cellule {i}\")\n",
    "                    del cell['metadata'][key]\n",
    "                    changes_made = True\n",
    "\n",
    "    # 3. Supprimer compl√®tement la section widgets s'il y en a une au niveau racine\n",
    "    widgets_keys = ['widgets', 'widget_state']\n",
    "    for key in widgets_keys:\n",
    "        if key in notebook:\n",
    "            print(f\"‚ùå Suppression de la cl√© {key} au niveau racine\")\n",
    "            del notebook[key]\n",
    "            changes_made = True\n",
    "\n",
    "    # 4. Sauvegarder\n",
    "    if changes_made:\n",
    "        with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(notebook, f, indent=1, ensure_ascii=False)\n",
    "        print(f\"‚úÖ {notebook_path} nettoy√© avec succ√®s!\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Aucun changement n√©cessaire\")\n",
    "\n",
    "    return changes_made\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "deep_clean_notebook('sam_Blip_HLBB.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_FW9JxorXLbm"
   },
   "source": [
    "**generation du HLBB en utilisant une fonction d'inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86,
     "referenced_widgets": [
      "809c3b4860e04b319de5511ecb252988",
      "b4eae5df81c6415ebb8c095b358a820e",
      "4b234e2aad6a4ec195da8b0c14fd2df5",
      "c42f58799ee543d2b5fa5adabe1062dc",
      "d34bff1bf7ff4bd6a0cd486f16f48b5e",
      "05cc3f676314454ab7032e8ae51e9f79",
      "0392c25ebc3c4fa3b2d36b898ca676ba",
      "915d64c0541e4b3d9b2b0ac6f60fff53",
      "0fcb2aad805e40fa8461ef9137115ec9",
      "307599fa4bab4e3b9aac154181267b13",
      "a48ca7b6c0344cb987e01b16588b78b5"
     ]
    },
    "id": "ijBJfzI1Wmr0",
    "outputId": "93f3f94e-e4f5-479d-a3bb-a29ceb745c46"
   },
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Charger le mod√®le de captioning\n",
    "processor_blip = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model_blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_Gd7wnEVJFn"
   },
   "outputs": [],
   "source": [
    "def generate_caption_blip(image_pil):\n",
    "    inputs = processor_blip(image_pil, return_tensors=\"pt\").to(model_blip.device)\n",
    "    with torch.no_grad():\n",
    "        output = model_blip.generate(**inputs)\n",
    "    caption = processor_blip.decode(output[0], skip_special_tokens=True)\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "yfRpkHrAVRkR",
    "outputId": "e1058c56-4059-4c2d-ac85-a46d617566a0"
   },
   "outputs": [],
   "source": [
    "hlbb_list = []  # liste des objets avec features et captions\n",
    "\n",
    "for result in results:\n",
    "    raw_boxes = result[\"boxes\"].cpu().numpy()\n",
    "    raw_labels = result[\"text_labels\"]\n",
    "\n",
    "    for box, label in zip(raw_boxes, raw_labels):\n",
    "        features = extract_hl_features(image_np, box, image.size)\n",
    "        crop_pil = image.crop(tuple(map(int, box)))\n",
    "\n",
    "        # Use the BLIP model (loaded in cell GwkPsJyayCi8) to generate the caption\n",
    "        caption = generate_caption_blip(crop_pil)\n",
    "\n",
    "        hlbb_list.append({\n",
    "            \"box\": [float(v) for v in box.tolist()],\n",
    "            \"label\": label,\n",
    "            \"caption\": caption,\n",
    "            \"features\": {\n",
    "                \"color_histogram\": [float(h) for h in features[\"color_histogram\"]],\n",
    "                \"texture_lbp\": [float(t) for t in features[\"texture_lbp\"]],\n",
    "                \"aspect_ratio\": float(features[\"aspect_ratio\"]),\n",
    "                \"relative_area\": float(features[\"relative_area\"])\n",
    "            }\n",
    "        })\n",
    "\n",
    "print(\"‚úÖ Extraction des HLBB + captions termin√©e.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUjfWoW_W44a"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_crop_with_caption(image_pil, box, caption):\n",
    "    x0, y0, x1, y1 = map(int, box)\n",
    "    crop = image_pil.crop((x0, y0, x1, y1))\n",
    "\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(crop)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(caption, fontsize=10)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Qc_L_yRGW7kN",
    "outputId": "a7ad537a-b44e-46b3-96f6-d17119a330c6"
   },
   "outputs": [],
   "source": [
    "# Pour chaque objet d√©tect√© dans hlbb_list\n",
    "for obj in hlbb_list:\n",
    "    box = obj[\"box\"]\n",
    "    x0, y0, x1, y1 = map(int, box)\n",
    "    crop = image.crop((x0, y0, x1, y1))\n",
    "\n",
    "    # G√©n√®re une description textuelle avec BLIP\n",
    "    caption = generate_caption_blip(crop)\n",
    "\n",
    "    # Affiche l‚Äôobjet + caption\n",
    "    show_crop_with_caption(image, box, caption)\n",
    "\n",
    "    # (optionnel) Ajoute la caption dans ton objet\n",
    "    obj[\"caption\"] = caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86o-Jy9HXpXA"
   },
   "outputs": [],
   "source": [
    "def get_position_description(box, image_size):\n",
    "    x0, y0, x1, y1 = map(int, box)\n",
    "    img_w, img_h = image_size\n",
    "    xc = (x0 + x1) / 2  # center x of the box\n",
    "    yc = (y0 + y1) / 2  # center y of the box\n",
    "\n",
    "    # Determine horizontal position\n",
    "    if xc < img_w / 3:\n",
    "        horizontal = \"on the left\"\n",
    "    elif xc < 2 * img_w / 3:\n",
    "        horizontal = \"in the center\"\n",
    "    else:\n",
    "        horizontal = \"on the right\"\n",
    "\n",
    "    # Determine vertical position\n",
    "    if yc < img_h / 3:\n",
    "        vertical = \"at the top\"\n",
    "    elif yc < 2 * img_h / 3:\n",
    "        vertical = \"in the middle\"\n",
    "    else:\n",
    "        vertical = \"at the bottom\"\n",
    "\n",
    "    return f\"{vertical} {horizontal}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B9uUY8UoXsMj"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def show_full_image_with_captions(image_pil, hlbb_list):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.imshow(image_pil)\n",
    "    image_size = image_pil.size\n",
    "\n",
    "    for idx, obj in enumerate(hlbb_list):\n",
    "        box = obj[\"box\"]\n",
    "        caption = obj.get(\"caption\", \"aucune description\")\n",
    "        x0, y0, x1, y1 = map(int, box)\n",
    "\n",
    "        # Rectangle\n",
    "        rect = patches.Rectangle((x0, y0), x1 - x0, y1 - y0, linewidth=2, edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        # Position relative dans l'image\n",
    "        pos_desc = get_position_description(box, image_size)\n",
    "\n",
    "        # Texte complet = position + caption\n",
    "        text = f\"{idx+1}. {pos_desc} : {caption}\"\n",
    "\n",
    "        # Affiche le texte juste au-dessus de la box\n",
    "        ax.text(x0, y0 - 10, text, fontsize=8, color='white', backgroundcolor='black')\n",
    "\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "9REcoA5OXujK",
    "outputId": "1bc11a71-fc9a-41d1-dbf8-f7b06d520e50"
   },
   "outputs": [],
   "source": [
    "# D‚Äôabord, g√©n√©rer les captions si ce n‚Äôest pas encore fait\n",
    "for obj in hlbb_list:\n",
    "    x0, y0, x1, y1 = map(int, obj[\"box\"])\n",
    "    crop = image.crop((x0, y0, x1, y1))\n",
    "    obj[\"caption\"] = generate_caption_blip(crop)\n",
    "\n",
    "# Ensuite, affiche tout dans l‚Äôimage compl√®te\n",
    "show_full_image_with_captions(image, hlbb_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BvzYIhlaYkOa"
   },
   "outputs": [],
   "source": [
    "def extract_caption_and_position(hlbb_list, image_size):\n",
    "    results = []\n",
    "    for obj in hlbb_list:\n",
    "        box = obj[\"box\"]\n",
    "        caption = obj.get(\"caption\", \"aucune description\")\n",
    "        position = get_position_description(box, image_size)\n",
    "        results.append({\n",
    "            \"position\": position,\n",
    "            \"caption\": caption\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wis3MFNbYgO8",
    "outputId": "aeeacb38-9bf2-40cb-c992-e7ae4211d047"
   },
   "outputs": [],
   "source": [
    "\n",
    "# R√©cup√©ration des positions + captions\n",
    "caption_position_list = extract_caption_and_position(hlbb_list, image.size)\n",
    "\n",
    "# Affichage au format demand√©\n",
    "for idx, item in enumerate(caption_position_list, 1):\n",
    "    print(f\"Objet {idx} is a \\\"{item['caption']}\\\" in {item['position']}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_YKiibG0wPJ",
    "outputId": "a03648e9-1515-45f6-dedb-f00bc88662ef"
   },
   "outputs": [],
   "source": [
    "!pip install nbconvert\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzDRaHl3lsOS"
   },
   "source": [
    "CLearing the notebook in order to use it in github\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHZ6vADH06lT"
   },
   "outputs": [],
   "source": [
    "!pip install -q nbformat nbconvert\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X43r0I5Oln7c",
    "outputId": "14a56b99-8942-41a6-932e-777c78adb6b5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upje7UaEnlmO"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "file = \"/content/sam_Blip_VF.ipynb\"  # name of your notebook\n",
    "\n",
    "# Open the notebook\n",
    "with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Check and delete 'widgets' from metadata\n",
    "if \"widgets\" in nb.metadata:\n",
    "    print(\"üîß Found 'metadata.widgets' ‚Äî removing it...\")\n",
    "    del nb.metadata[\"widgets\"]\n",
    "else:\n",
    "    print(\"‚úÖ No widgets found.\")\n",
    "\n",
    "# Save cleaned notebook\n",
    "with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"‚úÖ Notebook cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mx2EDA9RnlmP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JcWB9FfnlmP"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "file = \"/content/sam_Blip_VF.ipynb\"  # name of your notebook\n",
    "\n",
    "# Open the notebook\n",
    "with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "    nb = nbformat.read(f, as_version=4)\n",
    "\n",
    "# Check and delete 'widgets' from metadata\n",
    "if \"widgets\" in nb.metadata:\n",
    "    print(\"üîß Found 'metadata.widgets' ‚Äî removing it...\")\n",
    "    del nb.metadata[\"widgets\"]\n",
    "else:\n",
    "    print(\"‚úÖ No widgets found.\")\n",
    "\n",
    "# Save cleaned notebook\n",
    "with open(file, \"w\", encoding=\"utf-8\") as f:\n",
    "    nbformat.write(nb, f)\n",
    "\n",
    "print(\"‚úÖ Notebook cleaned and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gfc7XJYSnlmP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "\n",
    "def clean_notebook(notebook_path):\n",
    "    with open('/content/sam_Blip_VF.ipynb', 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    # Supprimer les m√©tadonn√©es de widgets au niveau du notebook\n",
    "    if 'widgets' in notebook.get('metadata', {}):\n",
    "        del notebook['metadata']['widgets']\n",
    "\n",
    "    # Nettoyer chaque cellule\n",
    "    for cell in notebook.get('cells', []):\n",
    "        # Supprimer les outputs des cellules\n",
    "        if 'outputs' in cell:\n",
    "            cell['outputs'] = []\n",
    "\n",
    "        # Supprimer les m√©tadonn√©es de widgets des cellules\n",
    "        if 'widgets' in cell.get('metadata', {}):\n",
    "            del cell['metadata']['widgets']\n",
    "\n",
    "        # R√©initialiser execution_count\n",
    "        if 'execution_count' in cell:\n",
    "            cell['execution_count'] = None\n",
    "\n",
    "    # Sauvegarder le notebook nettoy√©\n",
    "    with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(notebook, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"‚úÖ Notebook {notebook_path} nettoy√© avec succ√®s!\")\n",
    "\n",
    "# Utilisation\n",
    "clean_notebook('votre_notebook.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKq3VYD-nlmP"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def deep_clean_notebook(notebook_path):\n",
    "    print(f\"üîç Analyse de {notebook_path}...\")\n",
    "\n",
    "    with open('/content/sam_Blip_VF.ipynb', 'r', encoding='utf-8') as f:\n",
    "        notebook = json.load(f)\n",
    "\n",
    "    changes_made = False\n",
    "\n",
    "    # 1. Nettoyer les m√©tadonn√©es au niveau du notebook\n",
    "    if 'metadata' in notebook:\n",
    "        if 'widgets' in notebook['metadata']:\n",
    "            print(\"‚ùå Suppression des m√©tadonn√©es widgets du notebook\")\n",
    "            del notebook['metadata']['widgets']\n",
    "            changes_made = True\n",
    "\n",
    "        # Supprimer aussi d'autres m√©tadonn√©es probl√©matiques\n",
    "        problematic_keys = ['widget_state', 'application/vnd.jupyter.widget-state+json']\n",
    "        for key in problematic_keys:\n",
    "            if key in notebook['metadata']:\n",
    "                del notebook['metadata'][key]\n",
    "                changes_made = True\n",
    "\n",
    "    # 2. Nettoyer chaque cellule\n",
    "    for i, cell in enumerate(notebook.get('cells', [])):\n",
    "        # Supprimer les outputs\n",
    "        if 'outputs' in cell and cell['outputs']:\n",
    "            print(f\"üßπ Nettoyage des outputs de la cellule {i}\")\n",
    "            cell['outputs'] = []\n",
    "            changes_made = True\n",
    "\n",
    "        # Supprimer execution_count\n",
    "        if 'execution_count' in cell and cell['execution_count'] is not None:\n",
    "            cell['execution_count'] = None\n",
    "            changes_made = True\n",
    "\n",
    "        # Nettoyer les m√©tadonn√©es des cellules\n",
    "        if 'metadata' in cell:\n",
    "            cell_metadata_keys_to_remove = ['widgets', 'widget_state', 'application/vnd.jupyter.widget-state+json']\n",
    "            for key in cell_metadata_keys_to_remove:\n",
    "                if key in cell['metadata']:\n",
    "                    print(f\"‚ùå Suppression des m√©tadonn√©es {key} de la cellule {i}\")\n",
    "                    del cell['metadata'][key]\n",
    "                    changes_made = True\n",
    "\n",
    "    # 3. Supprimer compl√®tement la section widgets s'il y en a une au niveau racine\n",
    "    widgets_keys = ['widgets', 'widget_state']\n",
    "    for key in widgets_keys:\n",
    "        if key in notebook:\n",
    "            print(f\"‚ùå Suppression de la cl√© {key} au niveau racine\")\n",
    "            del notebook[key]\n",
    "            changes_made = True\n",
    "\n",
    "    # 4. Sauvegarder\n",
    "    if changes_made:\n",
    "        with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(notebook, f, indent=1, ensure_ascii=False)\n",
    "        print(f\"‚úÖ {notebook_path} nettoy√© avec succ√®s!\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Aucun changement n√©cessaire\")\n",
    "\n",
    "    return changes_made\n",
    "\n",
    "# Appliquer le nettoyage\n",
    "deep_clean_notebook('sam_Blip_HLBB.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fcbvX4d7meQ-",
    "outputId": "5aba66b2-c888-45d0-dfac-0244412d8915"
   },
   "outputs": [],
   "source": [
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace /content/Segma_Vision_Synchronizer_clean.ipynb\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}